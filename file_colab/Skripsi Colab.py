# -*- coding: utf-8 -*-
"""FIKS 128 lr 0.01  tanpa balancing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11M_PdArDds3PpjJmzWoVlJtV-0tD_jAF
"""

!pip install scikit-learn

!pip install contractions

import pandas as pd
import numpy as np

"""##Labelling Data & Preprocessing"""

selected_columns = ['Summary', 'Priority']
data_terlabeli = pd.read_csv('bugs-2024-12-14_libreoffice.csv', sep=';', usecols=selected_columns)

import nltk
import re
import contractions
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)

Lemmatizer = WordNetLemmatizer()

important_words = {'not', 'no', 'don’t', 'cannot'}
custom_stopwords = set(stopwords.words('english')) - important_words

def show_preprocessing_steps(text):
    if not isinstance(text, str):
        return "", []

    text = contractions.fix(text)
    text = text.lower()
    text = re.sub(r"#(\w+)", r"\1", text)
    text = re.sub(r"\b\d+\b", "", text)
    text = re.sub(r"[^a-zA-Z\s.-]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()

    words = text.split()
    combined_words = []
    skip_next = False

    for i in range(len(words)):
        if skip_next:
            skip_next = False
            continue
        if words[i] in {'not', 'no','don’t', 'cannot'} and i + 1 < len(words):
            combined = words[i] + '_' + words[i + 1]
            combined_words.append(combined)
            skip_next = True
        elif words[i] not in custom_stopwords and len(words[i]) > 2:
            combined_words.append(words[i])

    lemmatized_words = [Lemmatizer.lemmatize(word) for word in combined_words]

    processed_text = ' '.join(lemmatized_words)
    return processed_text, lemmatized_words

def categorize_priority(priority):
  if priority in ['high', 'highest']:
    return 'Tinggi'
  elif priority == 'medium':
    return 'Sedang'
  elif priority in ['low', 'lowest']:
    return 'Rendah'
  else:
    return None

data_terlabeli['Priority'] = data_terlabeli['Priority'].apply(categorize_priority)
data_terlabeli = data_terlabeli.dropna(subset=['Priority'])
processed_results = data_terlabeli['Summary'].apply(show_preprocessing_steps)
data_terlabeli['Processed_Summary'] = processed_results.apply(lambda x: x[0])
data_terlabeli['tokens'] = processed_results.apply(lambda x: x[1])

print(data_terlabeli[['Summary', 'Processed_Summary']].head(10))

priority_counts = data_terlabeli['Priority'].value_counts()
print("Distribusi Priority pada data terlabeli:")
print(priority_counts)
print(f"Total data terlabeli: {len(data_terlabeli)}")

print(data_terlabeli)

sample_rendah = data_terlabeli[data_terlabeli['Priority'] == 'Rendah'].sample(n=300, random_state=42)
sample_sedang = data_terlabeli[data_terlabeli['Priority'] == 'Sedang'].sample(n=300, random_state=42)
sample_tinggi = data_terlabeli[data_terlabeli['Priority'] == 'Tinggi'].sample(n=300, random_state=42)

data_sample = pd.concat([sample_rendah, sample_sedang, sample_tinggi], ignore_index=True)

selected_columns = ['Summary']
df1 = pd.read_csv('bugs-2024-12-14_ASF.csv', sep=';', usecols=selected_columns)
df2 = pd.read_csv('bugs-2024-12-14_eclipseide.csv', sep=';', usecols=selected_columns)
df3 = pd.read_csv('bugs-2024-12-14_linux kernel.csv', sep=';', usecols=selected_columns)
df4 = pd.read_csv('bugs-mozilla.csv', sep=';', usecols=selected_columns)

df_unlabeled = pd.concat([df1, df2, df3, df4], ignore_index=True)
processed_results_un = df_unlabeled['Summary'].apply(show_preprocessing_steps)
df_unlabeled['Processed_Summary'] = processed_results_un.apply(lambda x: x[0])
df_unlabeled['tokens'] = processed_results_un.apply(lambda x: x[1])

print(df_unlabeled[['Summary', 'Processed_Summary']].head(10))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import classification_report

def predict_priority_improved(unlabeled_df, labeled_df, max_features, threshold, min_df):
    # Cari label yang paling sering muncul untuk default
    priority_counts = labeled_df['Priority'].value_counts()
    default_priority = priority_counts.idxmax()

    # Buat TF-IDF dengan parameter tertentu
    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, min_df=min_df)
    labeled_tfidf = tfidf_vectorizer.fit_transform(labeled_df['Processed_Summary'])
    unlabeled_tfidf = tfidf_vectorizer.transform(unlabeled_df['Processed_Summary'])

    # Hitung kemiripan cosine
    similarities = cosine_similarity(unlabeled_tfidf, labeled_tfidf)

    predicted_priorities = []
    for i in range(len(unlabeled_df)):
        max_sim_idx = np.argmax(similarities[i])
        if similarities[i][max_sim_idx] > threshold:
            predicted_priorities.append(labeled_df.iloc[max_sim_idx]['Priority'])
        else:
            predicted_priorities.append(default_priority)

    return predicted_priorities

from sklearn.model_selection import train_test_split
train_data, test_data = train_test_split(data_sample, test_size=0.3, random_state=42)

def evaluate_grid_search(data):
    results = []
    # Rentang nilai parameter yang ingin dicoba
    max_features_values = [1000, 2000, 3000, 4000, 5000]
    threshold_values = [0.1, 0.2, 0.3, 0.4, 0.5]
    min_df_values = [1, 2, 3, 4, 5]

    for max_feat in max_features_values:
        for threshold in threshold_values:
            for min_df in min_df_values:
                preds = predict_priority_improved(test_data, train_data, max_feat, threshold, min_df)
                report = classification_report(test_data['Priority'], preds, output_dict=True, zero_division=1)
                f1 = report['macro avg']['f1-score']
                acc = report['accuracy']
                results.append({
                    'max_features': max_feat,
                    'threshold': threshold,
                    'min_df': min_df,
                    'f1_score': f1,
                    'accuracy': acc
                })

    result_df = pd.DataFrame(results)
    best_by_f1 = result_df.sort_values('f1_score', ascending=False).iloc[0]
    best_by_acc = result_df.sort_values('accuracy', ascending=False).iloc[0]

    print("Best parameters by F1 score:\n", best_by_f1)
    print("\nBest parameters by Accuracy:\n", best_by_acc)
    return result_df, best_by_f1, best_by_acc

results_df, best_by_f1, best_by_acc = evaluate_grid_search(data_sample)

best_threshold = best_by_f1['threshold']
best_max_features = int(best_by_f1['max_features'])
best_min_df = int(best_by_f1['min_df'])

df_unlabeled['Priority'] = predict_priority_improved(
    df_unlabeled, data_sample,
    max_features=best_max_features,
    threshold=best_threshold,
    min_df=best_min_df)

priority_counts = df_unlabeled['Priority'].value_counts()
print("Distribusi Priority pada data terlabeli:")
print(priority_counts)
print(f"Total data terlabeli: {len(df_unlabeled)}")
print("\nDistribusi Persentase:")
print((priority_counts / len(df_unlabeled) * 100).round(2).astype(str) + '%')

data_terlabeli.to_csv('data_terlabeli.csv', sep=';', index=False)

df_unlabeled.to_csv('data_terlabeli_2.csv', sep=';', index=False)

"""##Data Cleaning & Tokenisasi Padding"""

combined_df = pd.concat([data_terlabeli[['Processed_Summary', 'Priority', 'tokens']],
                         df_unlabeled[['Processed_Summary', 'Priority', 'tokens']]],
                        ignore_index=True)

print(combined_df)

duplicates = combined_df[combined_df['Processed_Summary'].duplicated(keep=False)]
print(f"Jumlah data duplikat: {len(duplicates)}")

missing_values = combined_df.isnull().sum()
print(f"Jumlah data yang hilang: \n{missing_values}")

combined_df = combined_df.drop_duplicates(subset=['Processed_Summary'], keep='first')
combined_df = combined_df.dropna()

total_data_awal = len(data_terlabeli) + len(df_unlabeled)
jumlah_setelah_bersih = len(combined_df)

print(f"Jumlah data awal: {total_data_awal}")
print(f"Jumlah data setelah pembersihan: {len(combined_df)}")
print(f"Persentase data yang hilang: {((total_data_awal - jumlah_setelah_bersih) / total_data_awal) * 100:.2f}%")

print(f"Distribusi label: \n{combined_df['Priority'].value_counts()}")

from tensorflow.keras.preprocessing.sequence import pad_sequences

all_tokens = []
for tokens in combined_df['tokens']:
    if tokens:
        all_tokens.extend(tokens)

unique_tokens = sorted(set(all_tokens))
vocab = {token: idx+1 for idx, token in enumerate(unique_tokens)}
vocab['<OOV>'] = len(vocab) + 1

print(f"Vocabulary size: {len(vocab)}")

def tokens_to_sequences(tokens, vocab):
    return [vocab.get(token, vocab['<OOV>']) for token in tokens]

combined_df['sequences'] = combined_df['tokens'].apply(lambda x: tokens_to_sequences(x, vocab))

seq_lengths = [len(seq) for seq in combined_df['sequences']]
print(f"Rata-rata panajang sequence:{np.mean(seq_lengths):.2f}")
print(f"Median panajang sequence:{np.median(seq_lengths)}")
print(f"Maksimum panajang sequence:{np.max(seq_lengths)}")

max_length = min(50, int(np.percentile(seq_lengths, 95)))
padded_sequences = pad_sequences(combined_df['sequences'], maxlen=max_length, padding='post')

combined_df

import pickle

# Simpan dataframe lengkap dengan sequence/token
with open('combined_df.pkl', 'wb') as f:
    pickle.dump(combined_df, f)

# Vocabulary
with open('vocabulary.pkl', 'wb') as f:
    pickle.dump(vocab, f)

# max_length
with open('max_length.txt', 'w') as f:
    f.write(str(max_length))

import pickle

# Load combined_df
with open('combined_df.pkl', 'rb') as f:
    combined_df = pickle.load(f)

# Load tokenizer
#with open('tokenizer.pkl', 'rb') as f:
 #   tokenizer = pickle.load(f)

# Load max_length
#with open('max_length.txt', 'r') as f:
 #   max_length = int(f.read())

"""##EDA"""

import matplotlib.pyplot as plt
from wordcloud import WordCloud

def generate_wordcloud(text, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=16)
    plt.show()

for priority_label in ['Rendah', 'Sedang', 'Tinggi']:
    priority_data = combined_df[combined_df['Priority'] == priority_label]
    text = ' '.join(priority_data['Processed_Summary'])
    generate_wordcloud(text, f'Wordcloud Priority: {priority_label}')

from sklearn.feature_extraction.text import CountVectorizer

def get_top_n_words(corpus, n=None):
    vec = CountVectorizer().fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:n]

for priority_label in ['Rendah', 'Sedang', 'Tinggi']:
    priority_data = combined_df[combined_df['Priority'] == priority_label]
    top_words = get_top_n_words(priority_data['Processed_Summary'], n=30)
    print(f"Top 30 words for Priority: {priority_label}")
    for word, freq in top_words:
        print(f"{word}: {freq}")

"""
##Label Encoder"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(combined_df['Priority'])

print ("Label classes:", label_encoder.classes_)

label_mapping = {idx: label for idx, label in enumerate(label_encoder.classes_)}
print("Label mapping:", label_mapping)

"""##Modelling"""

X = padded_sequences
y = y_encoded

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional, SpatialDropout1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers

def create_lstm_model(input_length):
  optimizer = Adam(learning_rate=0.01)
  vocab_size = len(vocab) + 1

  model = Sequential()
  model.add(Embedding(input_dim=vocab_size, output_dim=50))
  model.add(SpatialDropout1D(0.2))
  model.add(LSTM(64, return_sequences=False))
  model.add(Dropout(0.1))
  model.add(Dense(32, activation='relu'))
  model.add(Dropout(0.1))
  model.add(Dense(len(label_encoder.classes_), activation='softmax'))

  model.build(input_shape=(None, input_length))
  model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
  return model

input_length = padded_sequences.shape[1]
model = create_lstm_model(input_length)
model.summary()

def create_stacked_lstm_model(input_length):
    optimizer = Adam(learning_rate=0.01)
    vocab_size = len(vocab) + 1

    model = Sequential()
    model.add(Embedding(input_dim=vocab_size + 1, output_dim=150))
    model.add(SpatialDropout1D(0.3))
    model.add(LSTM(32, return_sequences=True))
    model.add(LSTM(32, return_sequences=False))
    model.add(Dropout(0.2))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.build(input_shape=(None, input_length))
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

model1 = create_stacked_lstm_model(input_length)
model1.summary()

def create_bilstm_model(input_length):
    optimizer = Adam(learning_rate=0.01)
    vocab_size = len(vocab) + 1

    model = Sequential()
    model.add(Embedding(input_dim=vocab_size + 1, output_dim=100))
    model.add(SpatialDropout1D(0.2))
    model.add(Bidirectional(LSTM(128, return_sequences=False)))
    model.add(Dropout(0.2))
    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.build(input_shape=(None, input_length))
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

model2 = create_bilstm_model(input_length)
model2.summary()

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

import matplotlib.pyplot as plt
import seaborn as sns

"""##Evaluasi Model"""

# Cek shape data yang sudah di-pad
print(f"Shape padded_sequences: {padded_sequences.shape}")
print(f"Max_length yang digunakan: {max_length}")

# Cek apakah konsisten
X = np.array(padded_sequences)
print(f"Shape X: {X.shape}")
print(f"X.shape[1] (panjang sequence): {X.shape[1]}")

# Cek di beberapa fold
skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
for i, (train_idx, test_idx) in enumerate(skf.split(X, y)):
    X_train_fold = X[train_idx]
    print(f"Fold {i+1} - X_train_fold.shape: {X_train_fold.shape}")

"""###Parameter 3"""

def evaluate_model(X, y, model_fn, num_folds=3, save_plot_as='conf_matrix.png'):
    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

    fold_no = 1
    acc_scores = []
    prec_scores = []
    rec_scores = []
    f1_scores = []
    cm_list = []

    all_y_test = []
    all_y_pred = []

    for train_idx, test_idx in skf.split(X, y):
        print(f"\nTraining fold {fold_no}/{num_folds}")

        X_train_fold, X_test_fold = X[train_idx], X[test_idx]
        y_train_fold, y_test_fold = y[train_idx], y[test_idx]

        train_class_distribution = np.bincount(y_train_fold)
        test_class_distribution = np.bincount(y_test_fold)

        print(f"Distribusi kelas train fold {fold_no}: {train_class_distribution}")
        print(f"Distribusi kelas test fold {fold_no}: {test_class_distribution}")

        model = model_fn(input_length=X_train_fold.shape[1])

        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)

        history = model.fit(
            X_train_fold, y_train_fold,
            epochs=30,
            batch_size=128,
            validation_data=(X_test_fold, y_test_fold),
            callbacks=[early_stopping, reduce_lr],
            verbose=1
        )

        y_pred_prob = model.predict(X_test_fold)
        y_pred = np.argmax(y_pred_prob, axis=1)

        all_y_test.extend(y_test_fold)
        all_y_pred.extend(y_pred)

        acc = accuracy_score(y_test_fold, y_pred)
        prec = precision_score(y_test_fold, y_pred, average='macro')
        rec = recall_score(y_test_fold, y_pred, average='macro')
        f1 = f1_score(y_test_fold, y_pred, average='macro')
        cm = confusion_matrix(y_test_fold, y_pred)

        acc_scores.append(acc)
        prec_scores.append(prec)
        rec_scores.append(rec)
        f1_scores.append(f1)
        cm_list.append(cm)

        print(f"Fold {fold_no} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}")
        print("Classification Report:")
        print(classification_report(y_test_fold, y_pred, target_names=label_encoder.classes_))

        fold_no += 1

    print("\n--- HASIL EVALUASI KESELURUHAN ---")
    print(f"Accuracy: {np.mean(acc_scores):.4f} ± {np.std(acc_scores):.4f}")
    print(f"Precision: {np.mean(prec_scores):.4f} ± {np.std(prec_scores):.4f}")
    print(f"Recall: {np.mean(rec_scores):.4f} ± {np.std(rec_scores):.4f}")
    print(f"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}")

    # Classification report untuk semua fold
    print("\nClassification Report (Keseluruhan):")
    print(classification_report(all_y_test, all_y_pred, target_names=label_encoder.classes_))

    # Plot average confusion matrix
    plt.figure(figsize=(10, 8))
    avg_cm = np.mean(cm_list, axis=0).astype(int)
    sns.heatmap(avg_cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Average Confusion Matrix Across All Folds')
    plt.tight_layout()
    plt.savefig(save_plot_as)

    return {
      'Accuracy': np.mean(acc_scores),
      'Precision': np.mean(prec_scores),
      'Recall': np.mean(rec_scores),
      'F1-Score': np.mean(f1_scores)
    }

result_lstm = evaluate_model(X, y, model_fn=create_lstm_model, save_plot_as='conf_matrix_lstm.png')

result_stacked = evaluate_model(X, y, model_fn=create_stacked_lstm_model, save_plot_as='conf_matrix_stacked.png')

result_bilstm = evaluate_model(X, y, model_fn=create_bilstm_model, save_plot_as='conf_matrix_bilstm.png')

param_desc = "3"

df_results = pd.DataFrame([
    {'Model': 'LSTM', **result_lstm, 'K': param_desc},
    {'Model': 'Stacked LSTM', **result_stacked, 'K': param_desc},
    {'Model': 'BiLSTM', **result_bilstm, 'K': param_desc}
])

print(df_results)

df_results.to_csv("hasil_perbandingan_model_K3.csv", index=False)

best_model = df_results.loc[df_results['F1-Score'].idxmax()]
print("\nModel terbaik secara keseluruhan:")
print(best_model)

"""###Parameter 5"""

def evaluate_model(X, y, model_fn, num_folds=5, save_plot_as='conf_matrix.png'):
    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

    fold_no = 1
    acc_scores = []
    prec_scores = []
    rec_scores = []
    f1_scores = []
    cm_list = []

    all_y_test = []
    all_y_pred = []

    for train_idx, test_idx in skf.split(X, y):
        print(f"\nTraining fold {fold_no}/{num_folds}")

        X_train_fold, X_test_fold = X[train_idx], X[test_idx]
        y_train_fold, y_test_fold = y[train_idx], y[test_idx]

        train_class_distribution = np.bincount(y_train_fold)
        test_class_distribution = np.bincount(y_test_fold)

        print(f"Distribusi kelas train fold {fold_no}: {train_class_distribution}")
        print(f"Distribusi kelas test fold {fold_no}: {test_class_distribution}")

        model = model_fn(input_length=X_train_fold.shape[1])

        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)

        history = model.fit(
            X_train_fold, y_train_fold,
            epochs=30,
            batch_size=128,
            validation_data=(X_test_fold, y_test_fold),
            callbacks=[early_stopping, reduce_lr],
            verbose=1
        )

        y_pred_prob = model.predict(X_test_fold)
        y_pred = np.argmax(y_pred_prob, axis=1)

        all_y_test.extend(y_test_fold)
        all_y_pred.extend(y_pred)

        acc = accuracy_score(y_test_fold, y_pred)
        prec = precision_score(y_test_fold, y_pred, average='macro')
        rec = recall_score(y_test_fold, y_pred, average='macro')
        f1 = f1_score(y_test_fold, y_pred, average='macro')
        cm = confusion_matrix(y_test_fold, y_pred)

        acc_scores.append(acc)
        prec_scores.append(prec)
        rec_scores.append(rec)
        f1_scores.append(f1)
        cm_list.append(cm)

        print(f"Fold {fold_no} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}")
        print("Classification Report:")
        print(classification_report(y_test_fold, y_pred, target_names=label_encoder.classes_))

        fold_no += 1

    print("\n--- HASIL EVALUASI KESELURUHAN ---")
    print(f"Accuracy: {np.mean(acc_scores):.4f} ± {np.std(acc_scores):.4f}")
    print(f"Precision: {np.mean(prec_scores):.4f} ± {np.std(prec_scores):.4f}")
    print(f"Recall: {np.mean(rec_scores):.4f} ± {np.std(rec_scores):.4f}")
    print(f"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}")

    # Classification report untuk semua fold
    print("\nClassification Report (Keseluruhan):")
    print(classification_report(all_y_test, all_y_pred, target_names=label_encoder.classes_))

    # Plot average confusion matrix
    plt.figure(figsize=(10, 8))
    avg_cm = np.mean(cm_list, axis=0).astype(int)
    sns.heatmap(avg_cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Average Confusion Matrix Across All Folds')
    plt.tight_layout()
    plt.savefig(save_plot_as)

    return {
      'Accuracy': np.mean(acc_scores),
      'Precision': np.mean(prec_scores),
      'Recall': np.mean(rec_scores),
      'F1-Score': np.mean(f1_scores)
    }

result_bilstm_K5 = evaluate_model(X, y, model_fn=create_bilstm_model, save_plot_as='conf_matrix_bilstm_k5.png')

"""###Parameter 10"""

def evaluate_model(X, y, model_fn, num_folds=10, save_plot_as='conf_matrix.png'):
    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

    fold_no = 1
    acc_scores = []
    prec_scores = []
    rec_scores = []
    f1_scores = []
    cm_list = []

    all_y_test = []
    all_y_pred = []

    for train_idx, test_idx in skf.split(X, y):
        print(f"\nTraining fold {fold_no}/{num_folds}")

        X_train_fold, X_test_fold = X[train_idx], X[test_idx]
        y_train_fold, y_test_fold = y[train_idx], y[test_idx]

        train_class_distribution = np.bincount(y_train_fold)
        test_class_distribution = np.bincount(y_test_fold)

        print(f"Distribusi kelas train fold {fold_no}: {train_class_distribution}")
        print(f"Distribusi kelas test fold {fold_no}: {test_class_distribution}")

        model = model_fn(input_length=X_train_fold.shape[1])

        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)

        history = model.fit(
            X_train_fold, y_train_fold,
            epochs=30,
            batch_size=128,
            validation_data=(X_test_fold, y_test_fold),
            callbacks=[early_stopping, reduce_lr],
            verbose=1
        )

        y_pred_prob = model.predict(X_test_fold)
        y_pred = np.argmax(y_pred_prob, axis=1)

        all_y_test.extend(y_test_fold)
        all_y_pred.extend(y_pred)

        acc = accuracy_score(y_test_fold, y_pred)
        prec = precision_score(y_test_fold, y_pred, average='macro')
        rec = recall_score(y_test_fold, y_pred, average='macro')
        f1 = f1_score(y_test_fold, y_pred, average='macro')
        cm = confusion_matrix(y_test_fold, y_pred)

        acc_scores.append(acc)
        prec_scores.append(prec)
        rec_scores.append(rec)
        f1_scores.append(f1)
        cm_list.append(cm)

        print(f"Fold {fold_no} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}")
        print("Classification Report:")
        print(classification_report(y_test_fold, y_pred, target_names=label_encoder.classes_))

        fold_no += 1

    print("\n--- HASIL EVALUASI KESELURUHAN ---")
    print(f"Accuracy: {np.mean(acc_scores):.4f} ± {np.std(acc_scores):.4f}")
    print(f"Precision: {np.mean(prec_scores):.4f} ± {np.std(prec_scores):.4f}")
    print(f"Recall: {np.mean(rec_scores):.4f} ± {np.std(rec_scores):.4f}")
    print(f"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}")

    # Classification report untuk semua fold
    print("\nClassification Report (Keseluruhan):")
    print(classification_report(all_y_test, all_y_pred, target_names=label_encoder.classes_))

    # Plot average confusion matrix
    plt.figure(figsize=(10, 8))
    avg_cm = np.mean(cm_list, axis=0).astype(int)
    sns.heatmap(avg_cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Average Confusion Matrix Across All Folds')
    plt.tight_layout()
    plt.savefig(save_plot_as)

    return {
      'Accuracy': np.mean(acc_scores),
      'Precision': np.mean(prec_scores),
      'Recall': np.mean(rec_scores),
      'F1-Score': np.mean(f1_scores)
    }

result_bilstm_K10 = evaluate_model(X, y, model_fn=create_bilstm_model,save_plot_as='conf_matrix_bilstm_k10.png')

df_results = pd.DataFrame([
    {'Model': 'BiLSTM 1', **result_bilstm, 'K': '3'},
    {'Model': 'BiLSTM 2', **result_bilstm_K5, 'K': '5'},
    {'Model': 'BiLSTM 3', **result_bilstm_K10, 'K': '10'}
])

print(df_results)

"""##Test Best Model"""

X_all = np.array(padded_sequences)
y_all = y_encoded

final_model = create_bilstm_model(input_length=X_all.shape[1])
final_model.fit(X_all, y_all, epochs=10, batch_size=128, verbose=1)

y_pred_prob = final_model.predict(X_all)
y_pred = np.argmax(y_pred_prob, axis=1)

accuracy = accuracy_score(y_all, y_pred)
precision = precision_score(y_all, y_pred, average=None)
recall = recall_score(y_all, y_pred, average=None)
f1 = f1_score(y_all, y_pred, average=None)

print(f"Accuracy: {accuracy:.4f}")
for i, (p, r, f) in enumerate(zip(precision, recall, f1)):
    print(f"Class {i} - Precision: {p:.4f}, Recall: {r:.4f}, F1-Score: {f:.4f}")

cm = confusion_matrix(y_all, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""##Test New Sentences"""

final_model.save('final_model.keras')
with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)

from tensorflow.keras.models import load_model
loaded_model = load_model('final_model.keras')
with open('label_encoder.pkl', 'rb') as f:
    loaded_label_encoder = pickle.load(f)

import nltk
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize

new_sentences_df = pd.read_csv('bugs-2025-05-04 - Copy.csv')
print(new_sentences_df.head())

new_sentences = new_sentences_df['Summary'].tolist()

new_processed_data = []
for sentence in new_sentences:
    processed_text, lemmatized_words = show_preprocessing_steps(sentence)
    new_processed_data.append(lemmatized_words)

new_tokens = new_processed_data

new_sequences = [tokens_to_sequences(tokens, vocab) for tokens in new_tokens]
new_padded_sequences = pad_sequences(new_sequences, maxlen=12, padding='post', truncating='post')

y_pred_prob = loaded_model.predict(new_padded_sequences)
y_pred = np.argmax(y_pred_prob, axis=1)

predicted_classes = loaded_label_encoder.inverse_transform(y_pred)

new_sentences_df['Predicted Class'] = predicted_classes
print(new_sentences_df)

# DEBUGGING: Lihat perbedaan preprocessing
print("\n=== DEBUGGING PREPROCESSING ===")
test_sentence = "Unable to add text . text boxes to any header/footer in Base"

_, new_tokens_sample = show_preprocessing_steps(test_sentence)
print(f"New method: {new_tokens_sample}")

new_sentences_df = pd.read_csv('bugs-2025-05-04 - Copy.csv')
print(new_sentences_df.head())

new_sentences = new_sentences_df['Summary'].tolist()

new_processed_data = []
for sentence in new_sentences:
    processed_text, lemmatized_words = show_preprocessing_steps(sentence)
    new_processed_data.append(lemmatized_words)

new_tokens = new_processed_data

new_sequences = [tokens_to_sequences(tokens, vocab) for tokens in new_tokens]
new_padded_sequences = pad_sequences(new_sequences, maxlen=12, padding='post', truncating='post')

y_pred_prob = loaded_model.predict(new_padded_sequences)
y_pred = np.argmax(y_pred_prob, axis=1)

predicted_classes = loaded_label_encoder.inverse_transform(y_pred)

new_sentences_df['Predicted Class'] = predicted_classes
print(new_sentences_df)

# DEBUGGING: Lihat perbedaan preprocessing
print("\n=== DEBUGGING PREPROCESSING ===")
test_sentence = "Unable to add text . text boxes to any header/footer in Base"

# Cara lama (salah)
old_tokens = word_tokenize(test_sentence.lower())
print(f"Old method: {old_tokens}")

# Cara baru (benar)
_, new_tokens_sample = show_preprocessing_steps(test_sentence)
print(f"New method: {new_tokens_sample}")

print(f"Same result: {old_tokens == new_tokens_sample}")